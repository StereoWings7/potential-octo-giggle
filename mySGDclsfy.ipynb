{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)\n",
    "#データの正規化\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "#データ数を取得\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "#ベクトル化\n",
    "x_train,x_test = x_train.reshape(n_train,-1),x_test.reshape(n_test,-1)\n",
    "#ノルムで標準化\n",
    "x_train = x_train / np.linalg.norm(x_train, ord=2, axis=1, keepdims=True)\n",
    "x_test = x_test / np.linalg.norm(x_test, ord=2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2値分類の設定とするため、猫と犬の画像データだけを抽出してくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つのクラスのみを選択する（例えば猫（3）と犬（5）を選択）\n",
    "classes_to_use = [3, 5]\n",
    "train_filter = np.isin(y_train, classes_to_use).flatten()\n",
    "test_filter = np.isin(y_test, classes_to_use).flatten()\n",
    "\n",
    "x_train_binary = x_train[train_filter]\n",
    "y_train_binary = y_train[train_filter]\n",
    "x_test_binary = x_test[test_filter]\n",
    "y_test_binary = y_test[test_filter]\n",
    "\n",
    "# ラベルを0と1に変換する（猫を0、犬を1とする）\n",
    "y_train_binary = (y_train_binary == classes_to_use[1]).astype(np.int32)\n",
    "y_test_binary = (y_test_binary == classes_to_use[1]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01990507 0.01881934 0.01755265 ... 0.00922871 0.00868585 0.01121922]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_binary[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純なロジスティック回帰. y_pred = 1/(1 + exp(1 + w.x)) ただしwはパラメータ、xは学習データのベクトル,ピリオドはこの2つの内積を示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ロジスティック回帰による二値分類\n",
    "class MyClassifier:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100,batch_size=1024):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        # 入力データをフラット化する\n",
    "        return data.reshape(data.shape[0], -1)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # シグモイド関数\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # 損失関数\n",
    "    def loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # データの前処理\n",
    "        X = self.preprocess(X)\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # イテレーション数の計算（全データを1回見渡すためのループ数）\n",
    "        iterate = num_samples // self.batch_size\n",
    "        print(\"iteration #:\",iterate)\n",
    "        if num_samples % self.batch_size != 0:\n",
    "            iterate += 1  # 端数を含む場合、追加で1イテレート\n",
    "\n",
    "        # 各エポックごとに、データをシャッフルしてバッチ学習\n",
    "        for epoch in trange(self.epochs, desc=\"current epoch\"):\n",
    "            # データをシャッフル\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            for iter in trange(iterate,desc=\"current iter\",leave=False):\n",
    "                # バッチごとに学習\n",
    "                start =  iter * self.batch_size\n",
    "                end = min(start + self.batch_size, num_samples)\n",
    "                #学習データをバッチサイズだけsliceしてくる。\n",
    "                X_batch = X[start:end]\n",
    "                #正解ラベルは元のデータからして縦ベクトル([num_samples,1]の形)なので、スライス後にsqueeze()で1次元にしないとy_predの計算がおかしい感じになる\n",
    "                y_batch = y[start:end].squeeze()\n",
    "\n",
    "                # バッチ全体での予測\n",
    "                # 線形モデルなのでz = w^T*x + bで計算\n",
    "                z = np.dot(X_batch, self.weights) + self.bias\n",
    "                # zをシグモイド関数に通すことで、予測値が0-1の範囲に収まる\n",
    "                y_pred = self.sigmoid(z)\n",
    "\n",
    "                # 勾配の計算\n",
    "                error = y_pred - y_batch\n",
    "                # パラメータの更新式は損失関数の微分から導かれる\n",
    "                # GéronテキストのLogistic Regressionの節を参照(σ(θ^T*xi)がy_pred,yiがy_batch,xiがX_batchに対応)\n",
    "                dw = np.dot(X_batch.T, error) / len(y_batch)\n",
    "                db = np.sum(error) / len(y_batch)\n",
    "\n",
    "                # パラメータの更新\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            #エポックごとに損失を表示\n",
    "            if(epoch % (self.epochs/10)== 0):\n",
    "                print(f'loss: {self.loss(y_pred,y_batch)} \\t')\n",
    "\n",
    "    def predict(self, X):\n",
    "        # データの前処理\n",
    "        X = self.preprocess(X)\n",
    "\n",
    "        # 線形モデルの予測\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(z)\n",
    "\n",
    "        # クラスの割り当て（0 または 1）\n",
    "        return (y_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:   0%|          | 1/1000 [00:00<03:14,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6952640023811486 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  10%|█         | 100/1000 [00:06<00:55, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6507083717831514 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  20%|██        | 200/1000 [00:13<00:50, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6428934144094256 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  30%|███       | 300/1000 [00:19<00:46, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6462564824346173 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  40%|████      | 400/1000 [00:26<00:41, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6569784001628274 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  50%|█████     | 500/1000 [00:32<00:31, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6729535612365051 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  60%|██████    | 600/1000 [00:39<00:25, 15.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6510146436036116 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  70%|███████   | 700/1000 [00:45<00:19, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6883034945707085 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  80%|████████  | 800/1000 [00:52<00:12, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6521184363002067 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch:  90%|█████████ | 900/1000 [00:58<00:06, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.628837359650653 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current epoch: 100%|██████████| 1000/1000 [01:05<00:00, 15.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# MyClassifierのインポートと初期化\n",
    "classifier = MyClassifier(learning_rate=0.1, epochs = 1000, batch_size=100)\n",
    "\n",
    "# トレーニング\n",
    "classifier.train(x_train_binary, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価\n",
    "y_pred = classifier.predict(x_test_binary)\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = np.mean(y_pred == y_test_binary.flatten())\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんどは単純な内積w.xじゃなくて、RBFを使って10000次元のベクトル$\\phi (x_i) = K(x_i,x_j)$に変換して、これに対してlogit損失関数で学習してみる。\n",
    "なぜ10000次元かというと、犬or猫の2値分類と化した問題でデータ数が10000個あるので、ある特定のデータが他のデータとどれくらい似てる?という組み合わせが10000通りあるから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ロジスティック回帰、RBFカーネルを導入した二値分類\n",
    "class MyClassifier_RBF:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, gamma=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = None  # RBFカーネルの重み\n",
    "        self.bias = 0\n",
    "        self.X_train = None  # トレーニングデータ全体を保持\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        # 入力データをフラット化する\n",
    "        return data.reshape(data.shape[0], -1)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # シグモイド関数\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def rbf_kernel(self, xi, xj):\n",
    "        # RBFカーネルの計算\n",
    "        return np.exp(-self.gamma * np.linalg.norm(xi - xj) ** 2)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # データの前処理\n",
    "        X = self.preprocess(X)\n",
    "        self.X_train = X\n",
    "        self.y_train = y.flatten()\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "\n",
    "        # 重みはトレーニングデータの各データポイントに対応（αを全データ数分用意）\n",
    "        self.alpha = np.zeros(num_samples)\n",
    "        self.bias = 0\n",
    "\n",
    "        # エポック数の計算（全データを1回見渡すための最小エポック数）\n",
    "        # 今度はすごく時間がかかることが予想されたので、データ全体をなめるのは1回だけになるようエポック数を設定した。\n",
    "        # 例: 10000データの場合、バッチサイズが100なら100エポックで全データをなめることになる\n",
    "        num_batches_per_epoch = num_samples // self.batch_size\n",
    "        if num_samples % self.batch_size != 0:\n",
    "            num_batches_per_epoch += 1  # 端数を含む場合、追加で1バッチ\n",
    "\n",
    "        # エポックごとに学習を実行\n",
    "        for epoch in trange(num_batches_per_epoch, desc=\"Training Progress\"):\n",
    "            # データをシャッフル\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # バッチごとに学習\n",
    "            start = (epoch * self.batch_size) % num_samples\n",
    "            end = min(start + self.batch_size, num_samples)\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "\n",
    "            # 各バッチ内の各データポイントについて学習\n",
    "            for i in range(len(X_batch)):\n",
    "                xi = X_batch[i]\n",
    "                yi = y_batch[i]\n",
    "\n",
    "                # RBFカーネルによる線形和の計算\n",
    "                z = 0\n",
    "                #他の10000データに対して類似度をRBFカーネルで計算し、重み係数αとの内積を取る\n",
    "                for j in range(num_samples):\n",
    "                    k_ij = self.rbf_kernel(xi, self.X_train[j])\n",
    "                    z += self.alpha[j] * k_ij\n",
    "                z += self.bias\n",
    "\n",
    "                # シグモイド関数を使って予測値を計算\n",
    "                y_pred = self.sigmoid(z)\n",
    "\n",
    "                # 勾配の計算\n",
    "                error = y_pred - yi\n",
    "                for j in range(num_samples):\n",
    "                    k_ij = self.rbf_kernel(xi, self.X_train[j])\n",
    "                    self.alpha[j] -= self.learning_rate * error * k_ij\n",
    "                self.bias -= self.learning_rate * error\n",
    "\n",
    "    def predict(self, X):\n",
    "        # データの前処理\n",
    "        X = self.preprocess(X)\n",
    "        num_samples = self.X_train.shape[0]\n",
    "        y_pred = []\n",
    "\n",
    "        # 各データポイントについて予測\n",
    "        for xi in X:\n",
    "            # RBFカーネルによる線形和の計算\n",
    "            z = 0\n",
    "            for j in range(num_samples):\n",
    "                k_ij = self.rbf_kernel(xi, self.X_train[j])\n",
    "                z += self.alpha[j] * k_ij\n",
    "            z += self.bias\n",
    "\n",
    "            # シグモイド関数を使って予測値を計算\n",
    "            prob = self.sigmoid(z)\n",
    "            y_pred.append(1 if prob >= 0.5 else 0)\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/313 [00:00<?, ?it/s]/var/folders/ts/jr_9pm0d7pq0vy46pp2qv7gh0000gp/T/ipykernel_54918/69679987.py:72: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.alpha[j] -= self.learning_rate * error * k_ij\n",
      "Training Progress: 100%|██████████| 313/313 [16:49<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# MyClassifierのインポートと初期化\n",
    "classifier = MyClassifier_RBF(learning_rate=0.01, batch_size=32)\n",
    "\n",
    "# トレーニング\n",
    "classifier.train(x_train_binary, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "# モデルの評価\n",
    "y_pred = classifier.predict(x_test_binary)\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = np.mean(y_pred == y_test_binary.flatten())\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16分かけてaccuracyが0.5とは。そもそもlearning_rateも適切な値がわからないし、RBFカーネルを使ってどうこうという話でもない。\n",
    "SVMの場合は、双対問題のほうでカーネルトリックが使えるので、陽にk_ij(写像した先での各成分)を計算する必要がない。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
